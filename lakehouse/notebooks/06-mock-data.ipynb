{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193dd100-d39b-433c-b6a2-2011024d5b50",
   "metadata": {},
   "source": [
    "### Setting up the Environment\n",
    "We begin by loading the utils.py file, which contains the necessary imports and functions to start a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3f0a8b-fbf8-48a7-b0f0-5527fb8c8d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/10 02:36:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/10 02:36:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession started with app name: Hudi-Notebooks\n"
     ]
    }
   ],
   "source": [
    "%run utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e0eb3a-bf78-438b-80d5-6a66a8e06005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession started with app name: Mock data\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session(\"Mock data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc448b5a-5eaa-4776-977b-6f50de9cdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"USE warehouse\") \n",
    "# spark.sql(\"DROP TABLE hudi.warehouse.vn30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36bc3330-fb10-44b5-b1ef-0922076933d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. sun.jvm.hotspot.memory.Universe.getNarrowOopBase()\n"
     ]
    }
   ],
   "source": [
    "df_vn30 = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/vn30.json\")\n",
    "df_vn30 = rename_df_columns(df_vn30)\n",
    "\n",
    "table_vn30 = \"vn30\"\n",
    "base_path_vn30 = f\"s3a://warehouse\"\n",
    "\n",
    "hudi_conf_vn30 = {\n",
    "    \"hoodie.table.name\": table_vn30, # The name of our Hudi table.\n",
    "    \"hoodie.database.name\": \"warehouse\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"stock_symbol\", # The column that acts as the unique identifier for each record.\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "    # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "    # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "    # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=new_york`.\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "}\n",
    "\n",
    "# Write the DataFrame to a Hudi COW table\n",
    "# The default operation is \"upsert\" if this is not specified.\n",
    "df_vn30.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf_vn30) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path_vn30}/{table_vn30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dde3ef7-9f95-453f-a74b-54bda502c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corporate_action = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/corporate-actions.json\")\n",
    "# df_corporate_action = rename_df_columns(df_corporate_action)\n",
    "\n",
    "# table_corporate_action = \"corporate_action\"\n",
    "# base_path_corporate_action = f\"s3a://warehouse\"\n",
    "\n",
    "# hudi_conf_corporate_action = {\n",
    "#     \"hoodie.table.name\": table_corporate_action, # The name of our Hudi table.\n",
    "#     \"hoodie.database.name\": \"warehouse\",\n",
    "#     \"hoodie.datasource.write.recordkey.field\": \"symbol\", # The column that acts as the unique identifier for each record.\n",
    "#     \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "#     # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "#     # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "#     # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "#     # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=new_york`.\n",
    "#     \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "#     \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "#     \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "#     \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "#     \"hoodie.schema.on.read.enable\": \"true\",\n",
    "# }\n",
    "\n",
    "# # Write the DataFrame to a Hudi COW table\n",
    "# # The default operation is \"upsert\" if this is not specified.\n",
    "# df_corporate_action.write \\\n",
    "#     .format(\"hudi\") \\\n",
    "#     .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "#     .options(**hudi_conf_corporate_action) \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .save(f\"{base_path_corporate_action}/{table_corporate_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a132ff-a82a-44ac-a47a-4c685305c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_industry = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/industry.json\")\n",
    "df_industry = rename_df_columns(df_industry)\n",
    "\n",
    "table_industry = \"industry\"\n",
    "base_path_industry = f\"s3a://warehouse\"\n",
    "\n",
    "hudi_conf_industry = {\n",
    "    \"hoodie.table.name\": table_industry, # The name of our Hudi table.\n",
    "    \"hoodie.database.name\": \"warehouse\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"icb_code\", # The column that acts as the unique identifier for each record.\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "    # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "    # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "    # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=new_york`.\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "}\n",
    "\n",
    "# Write the DataFrame to a Hudi COW table\n",
    "# The default operation is \"upsert\" if this is not specified.\n",
    "df_industry.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf_industry) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path_industry}/{table_industry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2080ad25-03ce-4776-9b4f-6f26736346f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stock_info = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/stock-info.json\")\n",
    "# df_stock_info = rename_df_columns(df_stock_info)\n",
    "\n",
    "# table_stock_info = \"stock_info\"\n",
    "# base_path_stock_info = f\"s3a://warehouse\"\n",
    "\n",
    "# hudi_conf_stock_info = {\n",
    "#     \"hoodie.table.name\": table_stock_info, # The name of our Hudi table.\n",
    "#     \"hoodie.database.name\": \"warehouse\",\n",
    "#     \"hoodie.datasource.write.recordkey.field\": \"symbol\", # The column that acts as the unique identifier for each record.\n",
    "#     \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "#     # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "#     # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "#     # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "#     # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=new_york`.\n",
    "#     \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "#     \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "#     \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "#     \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "#     \"hoodie.schema.on.read.enable\": \"true\",\n",
    "# }\n",
    "\n",
    "# # Write the DataFrame to a Hudi COW table\n",
    "# # The default operation is \"upsert\" if this is not specified.\n",
    "# df_stock_info.write \\\n",
    "#     .format(\"hudi\") \\\n",
    "#     .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "#     .options(**hudi_conf_stock_info) \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .save(f\"{base_path_stock_info}/{table_stock_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e086f5a0-024c-48f1-ad92-cb7c38fa46e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_basic = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/company-basic.json\")\n",
    "df_company_basic = rename_df_columns(df_company_basic)\n",
    "\n",
    "table_company_basic = \"company_basic\"\n",
    "base_path_company_basic = f\"s3a://warehouse\"\n",
    "\n",
    "hudi_conf_company_basic = {\n",
    "    \"hoodie.table.name\": table_company_basic, # The name of our Hudi table.\n",
    "    \"hoodie.database.name\": \"warehouse\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"ticker\", # The column that acts as the unique identifier for each record.\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "    # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "    # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "    # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=new_york`.\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "}\n",
    "\n",
    "# Write the DataFrame to a Hudi COW table\n",
    "# The default operation is \"upsert\" if this is not specified.\n",
    "df_company_basic.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf_company_basic) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path_company_basic}/{table_company_basic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f08c468-a65a-4620-91ed-c50f106fc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/news.json\")\n",
    "df_news = rename_df_columns(df_news)\n",
    "\n",
    "table_news = \"news\"\n",
    "base_path_news = f\"s3a://warehouse\"\n",
    "\n",
    "hudi_conf_news = {\n",
    "    \"hoodie.table.name\": table_news, # The name of our Hudi table.\n",
    "    \"hoodie.database.name\": \"warehouse\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"ticker\", # The column that acts as the unique identifier for each record.\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "    # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "    # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "    # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=new_york`.\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "}\n",
    "\n",
    "# Write the DataFrame to a Hudi COW table\n",
    "# The default operation is \"upsert\" if this is not specified.\n",
    "df_news.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf_news) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path_news}/{table_news}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e3faa2-3f89-4ad2-b556-533e4a9ac80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_fund = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/stocks-fund.json\")\n",
    "df_stock_fund = rename_df_columns(df_stock_fund)\n",
    "\n",
    "table_stock_fund = \"stock_fund\"\n",
    "base_path_stock_fund = f\"s3a://warehouse\"\n",
    "\n",
    "hudi_conf_stock_fund = {\n",
    "    \"hoodie.table.name\": table_stock_fund, # The name of our Hudi table.\n",
    "    \"hoodie.database.name\": \"warehouse\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"symbol\", # The column that acts as the unique identifier for each record.\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "    # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "    # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "    # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=stock_fund_york`.\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "}\n",
    "\n",
    "# Write the DataFrame to a Hudi COW table\n",
    "# The default operation is \"upsert\" if this is not specified.\n",
    "df_stock_fund.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf_stock_fund) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path_stock_fund}/{table_stock_fund}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96a4a1d-4f10-462d-a93f-5d87c8744d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = spark.read.option(\"multiLine\", \"true\").json(\"s3a://raw/event.json\")\n",
    "df_event = rename_df_columns(df_event)\n",
    "\n",
    "table_event = \"event\"\n",
    "base_path_event = f\"s3a://warehouse\"\n",
    "\n",
    "hudi_conf_event = {\n",
    "    \"hoodie.table.name\": table_event, # The name of our Hudi table.\n",
    "    \"hoodie.database.name\": \"warehouse\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id, symbol\", # The column that acts as the unique identifier for each record.\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\", # Hudi uses Copy-on-Write as the default table type, but we are being explicit here.\n",
    "    # \"hoodie.datasource.write.partitionpath.field\": \"city\", # The column Hudi uses to partition the data on storage.\n",
    "    # \"hoodie.datasource.write.precombine.field\": \"ts\", # The field used to deduplicate records when a conflict occurs.\n",
    "    # \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\", # This ensures partition directories are named like `city=event_york`.\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.jdbcurl\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "}\n",
    "\n",
    "# Write the DataFrame to a Hudi COW table\n",
    "# The default operation is \"upsert\" if this is not specified.\n",
    "df_event.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf_event) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path_event}/{table_event}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab41b01-bae6-4da4-a520-5ac21292aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3490eb-fa7d-4e54-9f66-0d8c3701a7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
